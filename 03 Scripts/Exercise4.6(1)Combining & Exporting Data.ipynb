{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71f934e-48bf-4a3f-b2d2-4f67ebdb05e8",
   "metadata": {},
   "source": [
    "# 4.6(1) Combining & Exporting Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66518751-aa7a-4c7e-8713-8354558564cb",
   "metadata": {},
   "source": [
    "### This script contains the folowing points:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b0635-00ac-4db3-917e-b4bcc260292c",
   "metadata": {},
   "source": [
    "### 01 Importing the data dictionaries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784b31e-c34e-4ba2-9e7c-e103ce3222df",
   "metadata": {},
   "source": [
    "### 02 Check the dimensions of the imported dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b58ee95-ede9-40ba-be65-e53e656a15fa",
   "metadata": {},
   "source": [
    "### 03.Determine a suitable way to combine the orders_products_combined dataframe with your products data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff80ac3-b6f4-4bb7-b331-b7b9f97b5e6d",
   "metadata": {},
   "source": [
    "### 04.Confirm the results of the merge using the merge flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d16a3c-78cb-4217-8663-2bbedee1f79e",
   "metadata": {},
   "source": [
    "### 05.Export the newly created dataframe as ords_prods_merge in a suitable format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a65eef6-225f-4739-9700-10751119b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8621a3-f8df-41b0-8e9a-5b9642e1a508",
   "metadata": {},
   "source": [
    "### 01. Importing Data dictionary order and products combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "828a81f2-71b1-4351-9b0e-f70100e658f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=(r\"C:\\Users\\hardjan\\Instacart Basket Analysis March 2024\")\n",
    "df_merged_large=pd.read_pickle(os.path.join(path, '02 Data','Prepared Data', 'orders_products_combined.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2c95e-1de4-49f3-8e45-921afc57bb65",
   "metadata": {},
   "source": [
    "### 02. check the dimensions of the imported dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8ba743d-e0cb-45e1-add4-c090a63b9ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32434489, 11)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7a0a1dc-1531-42b9-999a-3cd07ea15b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 512. KiB for an array with shape (65536,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#importing the orders and procuts combined.csv file \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_merged_large\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02 Data\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrepared Data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morders_products_combined.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:920\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1065\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1119\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1221\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1830\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 512. KiB for an array with shape (65536,) and data type int64"
     ]
    }
   ],
   "source": [
    "#importing the orders and procuts combined.csv file \n",
    "df_merged_large=pd.read_csv(os.path.join(path, '02 Data','Prepared Data', 'orders_products_combined.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86f426-4221-4e32-a662-d0a8c017e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the cleaned products dataset \n",
    "df_prods_clean_no_dups=pd.read_csv(os.path.join(path, '02 Data', 'Prepared Data', 'products_checked.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ab80d-0ce1-4faf-adf9-0339ae85bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_large.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c302aa8-6995-4b2e-b737-509f1bbf2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prods_clean_no_dups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e88168-e744-4577-979e-493fde12e486",
   "metadata": {},
   "source": [
    "### 03.Determine a suitable way to combine the orders_products_combined dataframe with your products data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c4ab6-899b-4e23-94b5-ef8b77a5a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_large_merge = df_prods_clean_no_dups.merge(df_merged_large, on = 'product_id', indicator = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b328b44-6513-432a-9d55-d5e78e2d89a8",
   "metadata": {},
   "source": [
    "#### I think the problem here is that the column \"merge\" already exists in the df_ords_prods_combined dataframe. Therefore, I have to drop it from the dataframe before operating the merging procedure.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160b2b6-3afa-4114-83e4-47a11ecf00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df_merged_large = df_merged_large.drop(['_merge'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c79a4-06e7-4825-bead-8b38f12bfd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check output\n",
    "df_merged_large.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3fbf0-e06e-4df9-b339-e666262e44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merged_large_merge = df_prods_clean_no_dups.merge(df_merged_large, on = 'product_id', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8671a-e281-463f-b007-d46608a6e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224b1c8-262a-4c90-aec6-1f042e0fc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the updated dataframes\n",
    "df_merged_large_merge = df_prods_clean_no_dups.merge(df_merged_large, on = 'product_id', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30bd99-377f-4805-9bc0-13e012f6fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_large_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a94e6e-9f67-4f8a-bb3a-2493d59e7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_large_merge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82954d89-edb8-4580-bc0f-3bca7c174e63",
   "metadata": {},
   "source": [
    "### 04 .Confirm the results of the merge using the merge flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9961a-6672-4dd7-917e-66bc5ab6fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_large_merge['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48e1d5-76aa-476a-a60d-f9364debae99",
   "metadata": {},
   "source": [
    "##### 07.The resulting dataframe (after the merge) has 32,404,859 rows, and each of those rows have information found in both input data sets, as we used an inner join for the purposes of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b57198-49da-4940-8b8c-935b30d7a7d3",
   "metadata": {},
   "source": [
    "### 05. Export the newly created dataframe as ords_prods_merge in a suitable format (taking into consideration the size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cd467-a53d-4ea6-beff-773ca94a1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to pkl\n",
    "df_merged_large_merge.to_pickle(os.path.join(path, '02 Data','Prepared Data', 'ords_prods_merge.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3ea1-229a-4daf-845c-ff9797ecc3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8645f-15ad-4c1c-b3ff-b683372f8594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66733c0-5b9b-4e93-922c-4b283b4ab4df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
